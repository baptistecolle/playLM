{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gymnasium\n",
    "# !pip install matplotlib\n",
    "# !pip install gym\n",
    "# !pip install pygame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "\n",
    "def remove_color(text):\n",
    "    ansi_escape = re.compile(r'\\x1B(?:[@-Z\\\\-_]|\\[[0-?]*[ -/]*[@-~])')\n",
    "    return ansi_escape.sub('', text)\n",
    "\n",
    "\n",
    "# launch adventure environment with human rendering\n",
    "env = gym.make('FrozenLake-v1', desc=None, map_name=\"4x4\", is_slippery=False, render_mode=\"ansi\")\n",
    "# todo make this random maps\n",
    "# render_mode=\"rgb_array\"\n",
    "env.reset()\n",
    "\n",
    "env_map = remove_color(env.render())\n",
    "\n",
    "number_of_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "'\\x1b[41mS\\x1b[0mFFF'\n",
      "(0, 0)\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "# def extract_ansi_positions(input_string):\n",
    "#     ansi_pattern = r'\\033\\[[0-9;]*m'\n",
    "#     ansi_matches = re.finditer(ansi_pattern, input_string)\n",
    "    \n",
    "#     positions = []\n",
    "#     for match in ansi_matches:\n",
    "#         start = match.start()\n",
    "#         end = match.end()\n",
    "#         positions.append((start, end))\n",
    "    \n",
    "#     return positions\n",
    "\n",
    "# env_map = env.render()\n",
    "# positions = extract_ansi_positions(env_map)\n",
    "# for position in positions:\n",
    "#     print(f\"Escaped string starts at position {position[0]} and ends at position {position[1]}\")\n",
    "\n",
    "# def get_position_based_on_color(env_map):\n",
    "\n",
    "#     # get size of map\n",
    "#     easy_map = env_map.split(\"\\n\")\n",
    "#     easy_map = list(map(lambda x: list(x), easy_map))\n",
    "\n",
    "#     print(easy_map)\n",
    "    \n",
    "#     size = len(map)\n",
    "\n",
    "#     # get position of player\n",
    "\n",
    "def extract_player_position(input_string):\n",
    "\n",
    "    lines = input_string.split('\\n')\n",
    "    # remove empty array elements\n",
    "\n",
    "    # remove empty lines\n",
    "    lines = list(filter(lambda x: x != '', lines))\n",
    "\n",
    "    # remove all lines with parenthesis\n",
    "    lines = list(filter(lambda x: '(' not in x, lines))\n",
    "\n",
    "\n",
    "    # for i in range(len(lines)):\n",
    "    #     if lines[i] != '' and lines[i] is not None:\n",
    "    #         lines_new.append(lines[i])\n",
    "        \n",
    "\n",
    "\n",
    "    size = len(lines)\n",
    "    \n",
    "    for i in range(size):\n",
    "        print(repr(lines[i]))\n",
    "        line = lines[i]\n",
    "        if '\\x1b' in line:\n",
    "            row = i\n",
    "            col = line.index('\\x1b')\n",
    "            return row, col\n",
    "    \n",
    "    return None  # Player position not found\n",
    "\n",
    "env_map = env.render()\n",
    "\n",
    "print(env_map)\n",
    "print(extract_player_position(env_map))\n",
    "\n",
    "\n",
    "\n",
    "print(type(env_map))\n",
    "\n",
    "\n",
    "# get_position_based_on_color(env_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "class LLM_Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = Llama(model_path=\"../model/wizardLM-7B.ggmlv3.q4_1.bin\", logits_all=True, verbose=False)\n",
    "        #, logits_all=True\n",
    "\n",
    "        # RL specific variables\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "\n",
    "        self.memory = [] # extract previous epsiodes from game\n",
    "        self.belief = [] # extract rules from game\n",
    "        self.actions_taken = [] # extract actions from game\n",
    "        self.insights = [\"\"] # extract insights from game\n",
    "        self.exploration_rate = 1.0 # exploration rate\n",
    "        self.map = None # extract map from game\n",
    "\n",
    "        self.character_prompt = \"\"\"\n",
    "        Imagine you are a player in a 2D puzzle game with a 4x4 square gridded map. \n",
    "        You are learning a game, you do not know the rules of the game but by playing the game you can learn the rules.\n",
    "        You should try to explore and always be willing to participate in the game.\n",
    "        \"\"\"\n",
    "\n",
    "        # You are located on the top left of the map and want to reach down right. (would this be cheating?)\n",
    "\n",
    "        self.values = \"\"\"\n",
    "        This is a list of your values:\n",
    "        - You should try to explore as much as possible and not go back to previous positions.\n",
    "        \"\"\"\n",
    "\n",
    "    def set_map(self, map):\n",
    "        self.map = map\n",
    "\n",
    "    def set_action_space(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.text_action_space = ', '.join(f\"'{w}'\" for w in action_space)\n",
    "        print(\"action space: \", self.action_space)\n",
    "\n",
    "    def set_character_prompt(self, character_prompt):\n",
    "        self.character_prompt = character_prompt\n",
    "\n",
    "    def save_observation(self, observation):\n",
    "        self.observations.append(observation)\n",
    "        print(f\"saved observation: {observation}\")\n",
    "\n",
    "    def save_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "\n",
    "    def decrease_exploration_rate(self, amount=0.1):\n",
    "        self.exploration_rate = max(0, self.exploration_rate - amount)\n",
    "\n",
    "    def get_action_prompt(self):\n",
    "        prompt = (\n",
    "            f\"\"\"\n",
    "            You are:\n",
    "            {self.character_prompt}\n",
    "            You can move in the following directions: {self.text_action_space}. \n",
    "            This is the map you are playing on:\n",
    "            {self.map}\n",
    "            {self.values}\n",
    "            This is your last observation about positioning in the game {self.observations[-1]}.\n",
    "            {self.insights[-1]}\n",
    "            You have a exploration rate of {self.exploration_rate * 100}%. If it is 100% you will just explore randomly (by choosing a random action), if it is 0% you will just use your insights.\n",
    "            What direction should the player move? \n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            # (please provide only the direction name, one word) \n",
    "           # If you are unsure, please output one of the directions listed before as you are learning how to play\n",
    "        #     \"Please output one of the possible actions. \"\n",
    "        #    \"Choose an action to take: \"\n",
    "        )\n",
    "\n",
    "        max_logprob = -100\n",
    "        max_action = None\n",
    "        probs = []\n",
    "\n",
    "        for action in self.action_space:\n",
    "            # print(f\"action: {action}\")\n",
    "            generation = self.llm(f\"\"\"{prompt}\n",
    "                          {action}\n",
    "                          \"\"\", logprobs=10, max_tokens=1, echo=True)\n",
    "            \n",
    "            tokens = generation['choices'][0]['logprobs']['tokens']\n",
    "            # print(f\"tokens: {tokens}\")\n",
    "            answer_index = tokens.index(\" Answer\")\n",
    "\n",
    "            action_index = tokens.index(f\" {action}\", answer_index)\n",
    "            action_logprob = generation['choices'][0]['logprobs']['token_logprobs'][action_index]\n",
    "            \n",
    "            action_in_list = tokens[action_index]\n",
    "            assert action_in_list == f\" {action}\", f\"action_in_list: {action_in_list}, action: {action}\"\n",
    "            # print(f\"action_logprob: {action_logprob}\")\n",
    "\n",
    "            probs.append(action_logprob)\n",
    "            # if (action_logprob > max_logprob):\n",
    "            #     max_logprob = action_logprob\n",
    "            #     max_action = action\n",
    "\n",
    "        # print(f\"probs: {probs}\")\n",
    "        probs = torch.nn.Softmax(dim=0)(torch.tensor(probs, dtype=torch.float))\n",
    "        # print(f\"probs: {probs}\")\n",
    "        max_action_index = torch.multinomial(probs, 1)\n",
    "        max_action = self.action_space[max_action_index]\n",
    "\n",
    "            \n",
    "        assert max_action is not None, \"max_action is None\" \n",
    "\n",
    "        self.actions_taken.append(max_action)\n",
    "\n",
    "        return max_action\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "   \n",
    "\n",
    "    def reflect(self):\n",
    "        print(\"reflecting\")\n",
    "\n",
    "        prompt = (\n",
    "            f\"\"\"\n",
    "            You are:\n",
    "            {self.character_prompt}\n",
    "            You can move in the following directions: {self.text_action_space}. \n",
    "            \n",
    "            You selected the following actions:\n",
    "            {self.actions_taken}.\n",
    "            You received the following rewards: \n",
    "            {self.rewards}.\n",
    "            This is the positions you got: \n",
    "            {self.observations}.\n",
    "            Please reflect on why you received the rewards you did.\n",
    "            Are you going in the right direction?\n",
    "            If not what direction should you go in?\n",
    "            Make an answer that is concise and actionable.\n",
    "            Answer:\n",
    "            \"\"\"\n",
    "            # (please provide only the direction name, one word) \n",
    "           # If you are unsure, please output one of the directions listed before as you are learning how to play\n",
    "        #     \"Please output one of the possible actions. \"\n",
    "        #    \"Choose an action to take: \"\n",
    "        )\n",
    "\n",
    "        print(\"reflecting prompt: \", prompt)\n",
    "\n",
    "        generation = self.llm(prompt)\n",
    "        insight = generation[\"choices\"][0][\"text\"]\n",
    "\n",
    "\n",
    "\n",
    "        print(f'generation: {insight}')\n",
    "\n",
    "        self.insights.append(insight)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        # did you get a reward \n",
    "        # is reward negative or positive\n",
    "        \n",
    "        # make hypothesis why you got reward\n",
    "        # did you move \n",
    "\n",
    "\n",
    "\n",
    "        pass\n",
    "        \n",
    "    def reflect_on_episode(self):\n",
    "\n",
    "        raise NotImplementedError\n",
    "\n",
    "        # self.insights = [\"\"] \n",
    "        # self.hypothesis\n",
    "        \"\"\" \n",
    "        Please make an hypothesis on the game rules based on the observations and rewards you received.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def reflect_on_game(self):\n",
    "        pass\n",
    "    \n",
    "    def generate_action(self, debug=False):\n",
    "        print(\"generate_action\")\n",
    "\n",
    "        action_word = self.get_action_prompt()\n",
    "\n",
    "\n",
    "        # convert action word to action number \n",
    "        action_number = self.action_space.index(action_word)\n",
    "\n",
    "        print(f\"action_word: {action_word}, action_number: {action_number}\")\n",
    "\n",
    "        return action_number\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random notes / ideas\n",
    "\n",
    "# multiple model controller, critic\n",
    "# queue and planning\n",
    "# long term memory \n",
    "# react \n",
    "# https://github.com/luca-medeiros/lang-segment-anything\n",
    "# autonomous agents text us back if does know what to do and learn from it\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../model/wizardLM-7B.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5809.34 MB (+ 1026.00 MB per state)\n",
      ".\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  ['left', 'down', 'right', 'up']\n"
     ]
    }
   ],
   "source": [
    "# Your a video game player, your goal is to get the key and get out of the dungeon\n",
    "\n",
    "agent = LLM_Agent()\n",
    "\n",
    "# TODO check if this actually does anything\n",
    "# env.action_space\n",
    "\n",
    "action_space = [ 'left', 'down', 'right', 'up']\n",
    "# action_space = env.action_space.seed(0)\n",
    "# action_space = []\n",
    "\n",
    "# for i in range(env.action_space.n):\n",
    "#     print(i)\n",
    "#     action_space.append(env.action_space.seed(i)[0])\n",
    "#     print(action_space)\n",
    "\n",
    "\n",
    "# print(\"action space: \", action_space)\n",
    "\n",
    "# how can we get the agent to learn the action space\n",
    "agent.set_action_space(action_space=action_space)\n",
    "\n",
    "agent.set_map(env_map)\n",
    "\n",
    "# agent.set_character_prompt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "observation: (0, {'prob': 1})\n",
      "saved observation: 0\n",
      "episode_reward: 0\n",
      "=> TIMESTAMP [0]; EPISODE [0]<=\n",
      "position: 0\n",
      "saved observation: 0\n",
      "generate_action\n",
      "action_word: up, action_number: 3\n",
      "selected action 3\n",
      "GENERATED ACTION:  up\n",
      "##################################################\n",
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "'\\x1b[41mS\\x1b[0mFFF'\n",
      "player_postion (0, 0)\n",
      "--------------------\n",
      "obs: 0\n",
      "reward: 0.0\n",
      "terminated: False\n",
      "truncated: False\n",
      "info: {'prob': 1.0}\n",
      "--------------------\n",
      "reflecting\n",
      "reflecting prompt:  \n",
      "            You are:\n",
      "            \n",
      "        Imagine you are a player in a 2D puzzle game with a 4x4 square gridded map. \n",
      "        You are learning a game, you do not know the rules of the game but by playing the game you can learn the rules.\n",
      "        You should try to explore and always be willing to participate in the game.\n",
      "        \n",
      "            You can move in the following directions: 'left', 'down', 'right', 'up'. \n",
      "            \n",
      "            You selected the following actions:\n",
      "            ['up'].\n",
      "            You received the following rewards: \n",
      "            [0.0].\n",
      "            This is the positions you got: \n",
      "            [0, 0].\n",
      "            Please reflect on why you received the rewards you did.\n",
      "            Are you going in the right direction?\n",
      "            If not what direction should you go in?\n",
      "            Make an answer that is concise and actionable.\n",
      "            Answer:\n",
      "            \n",
      "generation: \n",
      "        As an AI assistant, I cannot provide a definitive answer as I do not have access to the specific rules of the game you are playing. However, based on your actions and rewards received, it seems like you may be going in the wrong direction or not exploring the map thoroughly enough. To make progress in the game, it may be beneficial to explore other areas of the map and try moving in different directions besides 'up'.\n",
      "=> TIMESTAMP [1]; EPISODE [0]<=\n",
      "position: 0\n",
      "saved observation: 0\n",
      "generate_action\n",
      "action_word: down, action_number: 1\n",
      "selected action 1\n",
      "GENERATED ACTION:  down\n",
      "##################################################\n",
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "\n",
      "'SFFF'\n",
      "'\\x1b[41mF\\x1b[0mHFH'\n",
      "player_postion (1, 0)\n",
      "--------------------\n",
      "obs: 4\n",
      "reward: 0.0\n",
      "terminated: False\n",
      "truncated: False\n",
      "info: {'prob': 1.0}\n",
      "--------------------\n",
      "reflecting\n",
      "reflecting prompt:  \n",
      "            You are:\n",
      "            \n",
      "        Imagine you are a player in a 2D puzzle game with a 4x4 square gridded map. \n",
      "        You are learning a game, you do not know the rules of the game but by playing the game you can learn the rules.\n",
      "        You should try to explore and always be willing to participate in the game.\n",
      "        \n",
      "            You can move in the following directions: 'left', 'down', 'right', 'up'. \n",
      "            \n",
      "            You selected the following actions:\n",
      "            ['up', 'down'].\n",
      "            You received the following rewards: \n",
      "            [0.0, 0.0].\n",
      "            This is the positions you got: \n",
      "            [0, 0, 0].\n",
      "            Please reflect on why you received the rewards you did.\n",
      "            Are you going in the right direction?\n",
      "            If not what direction should you go in?\n",
      "            Make an answer that is concise and actionable.\n",
      "            Answer:\n",
      "            \n",
      "generation: \n",
      "        As an AI assistant, I do not have information about the game's rules or the purpose of the puzzle map. Therefore, it is difficult to determine why the player received the rewards they did or which direction they should go in. However, the player can explore different directions and try to collect more information about the game to make better decisions.\n",
      "=> TIMESTAMP [2]; EPISODE [0]<=\n",
      "position: 4\n",
      "saved observation: 4\n",
      "generate_action\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[78], line 48\u001b[0m\n\u001b[1;32m     31\u001b[0m agent\u001b[39m.\u001b[39msave_observation(position)\n\u001b[1;32m     33\u001b[0m \u001b[39m# if (position_old is not None) and (position_old != position):\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[39m#     # save observation\u001b[39;00m\n\u001b[1;32m     35\u001b[0m \u001b[39m#     agent.save_observation(f\"you are in position {position}. You moved from position {position_old}. Your last action was to go {actions_taken[-1]}.\")\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     46\u001b[0m \n\u001b[1;32m     47\u001b[0m \u001b[39m# generate action\u001b[39;00m\n\u001b[0;32m---> 48\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mgenerate_action(debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     49\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mselected action\u001b[39m\u001b[39m\"\u001b[39m, action)\n\u001b[1;32m     51\u001b[0m \u001b[39m# take action\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m# Within the game board state\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[75], line 194\u001b[0m, in \u001b[0;36mLLM_Agent.generate_action\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_action\u001b[39m(\u001b[39mself\u001b[39m, debug\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    192\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mgenerate_action\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 194\u001b[0m     action_word \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_action_prompt()\n\u001b[1;32m    197\u001b[0m     \u001b[39m# convert action word to action number \u001b[39;00m\n\u001b[1;32m    198\u001b[0m     action_number \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39mindex(action_word)\n",
      "Cell \u001b[0;32mIn[75], line 86\u001b[0m, in \u001b[0;36mLLM_Agent.get_action_prompt\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     82\u001b[0m probs \u001b[39m=\u001b[39m []\n\u001b[1;32m     84\u001b[0m \u001b[39mfor\u001b[39;00m action \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maction_space:\n\u001b[1;32m     85\u001b[0m     \u001b[39m# print(f\"action: {action}\")\u001b[39;00m\n\u001b[0;32m---> 86\u001b[0m     generation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm(\u001b[39mf\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mprompt\u001b[39m}\u001b[39;49;00m\n\u001b[1;32m     87\u001b[0m \u001b[39m                  \u001b[39;49m\u001b[39m{\u001b[39;49;00maction\u001b[39m}\u001b[39;49;00m\n\u001b[1;32m     88\u001b[0m \u001b[39m                  \u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m, logprobs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, max_tokens\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, echo\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     90\u001b[0m     tokens \u001b[39m=\u001b[39m generation[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mlogprobs\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtokens\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[1;32m     91\u001b[0m     \u001b[39m# print(f\"tokens: {tokens}\")\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/playlm/lib/python3.11/site-packages/llama_cpp/llama.py:1101\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model)\u001b[0m\n\u001b[1;32m   1058\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1059\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1060\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1077\u001b[0m     model: Optional[\u001b[39mstr\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1078\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m   1079\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1080\u001b[0m \n\u001b[1;32m   1081\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1099\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1100\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1101\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m   1102\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m   1103\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m   1104\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1105\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1106\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1107\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m   1108\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m   1109\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1110\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1111\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1112\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1113\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1114\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1115\u001b[0m         tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1116\u001b[0m         mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1117\u001b[0m         mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1118\u001b[0m         mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1119\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1120\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/playlm/lib/python3.11/site-packages/llama_cpp/llama.py:1055\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model)\u001b[0m\n\u001b[1;32m   1053\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1054\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1055\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1056\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/playlm/lib/python3.11/site-packages/llama_cpp/llama.py:945\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model)\u001b[0m\n\u001b[1;32m    942\u001b[0m text_offset \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(token_str)\n\u001b[1;32m    943\u001b[0m tokens\u001b[39m.\u001b[39mappend(token_str)\n\u001b[1;32m    944\u001b[0m sorted_logprobs \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\n\u001b[0;32m--> 945\u001b[0m     \u001b[39msorted\u001b[39m(\n\u001b[1;32m    946\u001b[0m         \u001b[39mzip\u001b[39m(logprobs_token, \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(logprobs_token))), reverse\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    947\u001b[0m     )\n\u001b[1;32m    948\u001b[0m )\n\u001b[1;32m    949\u001b[0m token_logprobs\u001b[39m.\u001b[39mappend(sorted_logprobs[\u001b[39mint\u001b[39m(token)][\u001b[39m0\u001b[39m])\n\u001b[1;32m    950\u001b[0m top_logprob: Optional[Dict[\u001b[39mstr\u001b[39m, \u001b[39mfloat\u001b[39m]] \u001b[39m=\u001b[39m {\n\u001b[1;32m    951\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize([i])\u001b[39m.\u001b[39mdecode(\u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m, errors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mignore\u001b[39m\u001b[39m\"\u001b[39m): logprob\n\u001b[1;32m    952\u001b[0m     \u001b[39mfor\u001b[39;00m logprob, i \u001b[39min\u001b[39;00m sorted_logprobs[:logprobs]\n\u001b[1;32m    953\u001b[0m }\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rewards = []\n",
    "observations = []\n",
    "actions_taken = []\n",
    "position_old = None\n",
    "cumulative_reward = 0\n",
    "\n",
    "def get_position(observation):\n",
    "    return observation[0]\n",
    "\n",
    "for i_episode in range(10):\n",
    "    observation = env.reset()\n",
    "    print(f\"observation: {observation}\")\n",
    "    observations.append(observation)\n",
    "    position = get_position(observations[-1])\n",
    "    agent.save_observation(position)\n",
    "\n",
    "    episode_reward = 0\n",
    "    print(f\"episode_reward: {episode_reward}\")\n",
    "\n",
    "    # Loop over t timesteps \n",
    "    for t in range(20):\n",
    "\n",
    "        print(f\"=> TIMESTAMP [{t}]; EPISODE [{i_episode}]<=\")\n",
    "\n",
    "        # Set position ot the last index in observations\n",
    "        position = get_position(observations[-1])\n",
    "        print(f\"position: {position}\")\n",
    "\n",
    "        # Give the LLM some information about the current state of the game\n",
    "\n",
    "        agent.save_observation(position)\n",
    "\n",
    "        # if (position_old is not None) and (position_old != position):\n",
    "        #     # save observation\n",
    "        #     agent.save_observation(f\"you are in position {position}. You moved from position {position_old}. Your last action was to go {actions_taken[-1]}.\")\n",
    "        # elif position_old is None:\n",
    "        \n",
    "        #     agent.save_observation(f\"you are in position {position}. This is the start of the game.\")\n",
    "        # else:\n",
    "        #     # save observation\n",
    "        #     agent.save_observation(f\"you are still in position {position}. You did not move. Your last action was to go {actions_taken[-1]}.\")\n",
    "        \n",
    "        \n",
    "        \n",
    "        # print(f\"saved observation: you are in position {position}\")\n",
    "\n",
    "        # generate action\n",
    "        action = agent.generate_action(debug=True)\n",
    "        print(\"selected action\", action)\n",
    "\n",
    "        # take action\n",
    "        # Within the game board state\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        print(\"GENERATED ACTION: \", action_space[action])\n",
    "        print(\"##################################################\")\n",
    "        # print(\"RENEDERED ACTION: \", env.render())\n",
    "        #show the image of the render of the action\n",
    "        print(env.render())\n",
    "        print(\"player_postion\", extract_player_position(env.render()))\n",
    "        # plt.imshow(env.render())\n",
    "        # plt.show()\n",
    "\n",
    "\n",
    "        print(\"--------------------\")\n",
    "        print(f\"obs: {obs}\")\n",
    "        print(f\"reward: {reward}\")\n",
    "        print(f\"terminated: {terminated}\")\n",
    "        print(f\"truncated: {truncated}\")\n",
    "        print(f\"info: {info}\")\n",
    "        print(\"--------------------\")\n",
    "\n",
    "        observations.append((obs, info))\n",
    "\n",
    "        # save reward\n",
    "        agent.save_reward(reward)\n",
    "\n",
    "        # reflect on action\n",
    "        agent.reflect()\n",
    "\n",
    "        cumulative_reward += reward\n",
    "\n",
    "        position_old = position\n",
    "\n",
    "        actions_taken.append(action_space[action])\n",
    "\n",
    "        # If the episode terminated prematurely, save the reward and stop the episode\n",
    "        if terminated or truncated:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            rewards.append(cumulative_reward)\n",
    "            cumulative_reward = 0\n",
    "            \n",
    "            agent.reflect_on_episode()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x41f99f4d0>]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# print(rewards)\n",
    "\n",
    "# set title\n",
    "\n",
    "plt.title(\"Rewards over episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Reward\")\n",
    "\n",
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
