{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting gymnasium\n",
      "  Downloading gymnasium-0.28.1-py3-none-any.whl (925 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m925.5/925.5 KB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting farama-notifications>=0.0.1\n",
      "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.3.0 in /home/rkargul/.local/lib/python3.10/site-packages (from gymnasium) (4.6.2)\n",
      "Collecting numpy>=1.21.0\n",
      "  Downloading numpy-1.24.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cloudpickle>=1.2.0\n",
      "  Downloading cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting jax-jumpy>=1.0.0\n",
      "  Downloading jax_jumpy-1.0.0-py3-none-any.whl (20 kB)\n",
      "Installing collected packages: farama-notifications, numpy, cloudpickle, jax-jumpy, gymnasium\n",
      "Successfully installed cloudpickle-2.2.1 farama-notifications-0.0.4 gymnasium-0.28.1 jax-jumpy-1.0.0 numpy-1.24.3\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting matplotlib\n",
      "  Downloading matplotlib-3.7.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hCollecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/rkargul/.local/lib/python3.10/site-packages (from matplotlib) (1.24.3)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.4-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil>=2.7 in /home/rkargul/.local/lib/python3.10/site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib) (2.4.7)\n",
      "Collecting fonttools>=4.22.0\n",
      "  Downloading fonttools-4.39.4-py3-none-any.whl (1.0 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib) (9.0.1)\n",
      "Collecting contourpy>=1.0.1\n",
      "  Downloading contourpy-1.0.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (300 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m300.3/300.3 KB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
      "Installing collected packages: kiwisolver, fonttools, cycler, contourpy, matplotlib\n",
      "Successfully installed contourpy-1.0.7 cycler-0.11.0 fonttools-4.39.4 kiwisolver-1.4.4 matplotlib-3.7.1\n"
     ]
    }
   ],
   "source": [
    "!pip install gymnasium\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# launch adventure environment with human rendering\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "env.reset()\n",
    "\n",
    "number_of_actions = env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from llama_cpp import Llama\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "\n",
    "class LLM_Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.llm = Llama(model_path=\"../model/wizardLM-7B.ggmlv3.q4_1.bin\", logits_all=True)\n",
    "\n",
    "        # RL specific variables\n",
    "        self.observations = []\n",
    "        self.rewards = []\n",
    "\n",
    "\n",
    "        self.memory = [] # extract previous epsiodes from game\n",
    "        self.belief = [] # extract rules from game\n",
    "\n",
    "    def set_action_space(self, action_space):\n",
    "        self.action_space = action_space\n",
    "        self.text_action_space = ', '.join(f\"'{w}'\" for w in action_space)\n",
    "        print(\"action space: \", self.action_space)\n",
    "\n",
    "    def get_action_prompt(self):\n",
    "        prompt = (\n",
    "            f\"{self.character_prompt}\"\n",
    "            f\"This is the current state of the board {self.observations[-1]}. \"\n",
    "            f\"You can use the following actions: {self.text_action_space}. \"\n",
    "            f\"Please output one of the possible actions. \"\n",
    "            f\"If you are unsure, please output an action as you are learning how to use the controllers of this video game. \"\n",
    "            f\"The selected action is: \"\n",
    "        )\n",
    "        return prompt\n",
    "\n",
    "    def set_character_prompt(self, character_prompt):\n",
    "        self.character_prompt = character_prompt\n",
    "\n",
    "    def generate(self, prompt):\n",
    "        return self.llm(f\"\"\"{prompt}\"\"\", echo=True)\n",
    "    \n",
    "    def save_observation(self, observation):\n",
    "        self.observations.append(observation)\n",
    "\n",
    "    def save_reward(self, reward):\n",
    "        self.rewards.append(reward)\n",
    "    \n",
    "    def generate_action(self, debug=False):\n",
    "\n",
    "        prompt = self.get_action_prompt()\n",
    "        print(f\"get_action_prompt: {prompt}\")\n",
    "\n",
    "        # TODO check actions are valid\n",
    "        output = self.generate(prompt)\n",
    "        print(f\"generated output: {output}\")\n",
    "        \n",
    "        generated_text = output['choices'][0]['text']\n",
    "        \n",
    "        # Find the direction chosen by the LLM_agent\n",
    "        generated_direction = re.search(\"'([^']+)'\", generated_text)\n",
    "        print(f\"Generated direction: {generated_direction.group(1)}\")\n",
    "\n",
    "\n",
    "        # TODO beam search would be better here or using raw logits\n",
    "        \n",
    "\n",
    "        # for all actions in action space check at which index it appears in the generated text\n",
    "        # the action with the lowest index is the action we want to take\n",
    "        text_index = 1000\n",
    "        action_number = -1\n",
    "        for i, current_action in enumerate(self.action_space):\n",
    "\n",
    "            action_index = generated_text.find(current_action)\n",
    "            \n",
    "            if action_index != -1 and action_index < text_index:\n",
    "                text_index = action_index\n",
    "                action_number = i\n",
    "                \n",
    "\n",
    "        if action_number == -1:\n",
    "            # raise Exception(\"No action found in generated text\")\n",
    "            # print warning in red\n",
    "            print(\"\\033[91mNo action found in generated text\\033[0m\")\n",
    "            # randomly select action\n",
    "            action_number = np.random.randint(0, len(self.action_space))\n",
    "        \n",
    "        return action_number\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some random notes / ideas\n",
    "\n",
    "# multiple model controller, critic\n",
    "# queue and planning\n",
    "# long term memory \n",
    "# react \n",
    "# https://github.com/luca-medeiros/lang-segment-anything\n",
    "# autonomous agents text us back if does know what to do and learn from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama.cpp: loading model from ../model/wizardLM-7B.ggmlv3.q4_1.bin\n",
      "llama_model_load_internal: format     = ggjt v3 (latest)\n",
      "llama_model_load_internal: n_vocab    = 32001\n",
      "llama_model_load_internal: n_ctx      = 512\n",
      "llama_model_load_internal: n_embd     = 4096\n",
      "llama_model_load_internal: n_mult     = 256\n",
      "llama_model_load_internal: n_head     = 32\n",
      "llama_model_load_internal: n_layer    = 32\n",
      "llama_model_load_internal: n_rot      = 128\n",
      "llama_model_load_internal: ftype      = 3 (mostly Q4_1)\n",
      "llama_model_load_internal: n_ff       = 11008\n",
      "llama_model_load_internal: n_parts    = 1\n",
      "llama_model_load_internal: model size = 7B\n",
      "llama_model_load_internal: ggml ctx size =    0.07 MB\n",
      "llama_model_load_internal: mem required  = 5809.34 MB (+ 1026.00 MB per state)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action space:  ['left', 'down', 'right', 'up']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      ".\n",
      "llama_init_from_file: kv self size  =  256.00 MB\n",
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | VSX = 0 | \n"
     ]
    }
   ],
   "source": [
    "# Your a video game player, your goal is to get the key and get out of the dungeon\n",
    "\n",
    "agent = LLM_Agent()\n",
    "\n",
    "env.action_space\n",
    "\n",
    "# how can we get the agent to learn the action space\n",
    "agent.set_action_space([\"left\", \"down\", \"right\", \"up\"])\n",
    "\n",
    "agent.set_character_prompt(\"You are a video game player.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> TIMESTAMP [0]; EPISODE [0]<=\n",
      "saved observation: you are in position 0\n",
      "get_action_prompt: You are a video game player.This is the current state of the board you are in position 0. You can use the following actions: 'left', 'down', 'right', 'up'. Please output one of the possible actions. If you are unsure, please output an action as you are learning how to use the controllers of this video game. The selected action is: \n",
      "generated output: {'id': 'cmpl-6d7789bb-adbc-4b46-9ea4-7e0d73eec344', 'object': 'text_completion', 'created': 1685373555, 'model': '../model/wizardLM-7B.ggmlv3.q4_1.bin', 'choices': [{'text': \"You are a video game player.This is the current state of the board you are in position 0. You can use the following actions: 'left', 'down', 'right', 'up'. Please output one of the possible actions. If you are unsure, please output an action as you are learning how to use the controllers of this video game. The selected action is: \\n```\\nYou have chosen the action 'left'.\", 'index': 0, 'logprobs': None, 'finish_reason': 'stop'}], 'usage': {'prompt_tokens': 80, 'completion_tokens': 11, 'total_tokens': 91}}\n",
      "Generated direction: left\n",
      "selected action 0\n",
      "=> TIMESTAMP [1]; EPISODE [0]<=\n",
      "saved observation: you are in position 0\n",
      "get_action_prompt: You are a video game player.This is the current state of the board you are in position 0. You can use the following actions: 'left', 'down', 'right', 'up'. Please output one of the possible actions. If you are unsure, please output an action as you are learning how to use the controllers of this video game. The selected action is: \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "llama_print_timings:        load time =  7328.34 ms\n",
      "llama_print_timings:      sample time =     9.12 ms /    12 runs   (    0.76 ms per token)\n",
      "llama_print_timings: prompt eval time =  7328.29 ms /    80 tokens (   91.60 ms per token)\n",
      "llama_print_timings:        eval time =  2890.59 ms /    11 runs   (  262.78 ms per token)\n",
      "llama_print_timings:       total time = 10674.46 ms\n",
      "Llama.generate: prefix-match hit\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39msaved observation: you are in position \u001b[39m\u001b[39m{\u001b[39;00mposition\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     19\u001b[0m \u001b[39m# generate action\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mgenerate_action(debug\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     21\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mselected action\u001b[39m\u001b[39m\"\u001b[39m, action)\n\u001b[1;32m     23\u001b[0m obs, reward, terminated, truncated, info \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[59], line 53\u001b[0m, in \u001b[0;36mLLM_Agent.generate_action\u001b[0;34m(self, debug)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mget_action_prompt: \u001b[39m\u001b[39m{\u001b[39;00mprompt\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[39m# TODO check actions are valid\u001b[39;00m\n\u001b[0;32m---> 53\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt)\n\u001b[1;32m     54\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgenerated output: \u001b[39m\u001b[39m{\u001b[39;00moutput\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     56\u001b[0m generated_text \u001b[39m=\u001b[39m output[\u001b[39m'\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m'\u001b[39m][\u001b[39m0\u001b[39m][\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\n",
      "Cell \u001b[0;32mIn[59], line 39\u001b[0m, in \u001b[0;36mLLM_Agent.generate\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate\u001b[39m(\u001b[39mself\u001b[39m, prompt):\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm(\u001b[39mf\u001b[39;49m\u001b[39m\"\"\"\u001b[39;49m\u001b[39m{\u001b[39;49;00mprompt\u001b[39m}\u001b[39;49;00m\u001b[39m\"\"\"\u001b[39;49m, echo\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1161\u001b[0m, in \u001b[0;36mLlama.__call__\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m   1116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\n\u001b[1;32m   1117\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   1118\u001b[0m     prompt: \u001b[39mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1137\u001b[0m     logits_processor: Optional[LogitsProcessorList] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   1138\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Union[Completion, Iterator[CompletionChunk]]:\n\u001b[1;32m   1139\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Generate text from a prompt.\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \n\u001b[1;32m   1141\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1159\u001b[0m \u001b[39m        Response object containing the generated text.\u001b[39;00m\n\u001b[1;32m   1160\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1161\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcreate_completion(\n\u001b[1;32m   1162\u001b[0m         prompt\u001b[39m=\u001b[39;49mprompt,\n\u001b[1;32m   1163\u001b[0m         suffix\u001b[39m=\u001b[39;49msuffix,\n\u001b[1;32m   1164\u001b[0m         max_tokens\u001b[39m=\u001b[39;49mmax_tokens,\n\u001b[1;32m   1165\u001b[0m         temperature\u001b[39m=\u001b[39;49mtemperature,\n\u001b[1;32m   1166\u001b[0m         top_p\u001b[39m=\u001b[39;49mtop_p,\n\u001b[1;32m   1167\u001b[0m         logprobs\u001b[39m=\u001b[39;49mlogprobs,\n\u001b[1;32m   1168\u001b[0m         echo\u001b[39m=\u001b[39;49mecho,\n\u001b[1;32m   1169\u001b[0m         stop\u001b[39m=\u001b[39;49mstop,\n\u001b[1;32m   1170\u001b[0m         frequency_penalty\u001b[39m=\u001b[39;49mfrequency_penalty,\n\u001b[1;32m   1171\u001b[0m         presence_penalty\u001b[39m=\u001b[39;49mpresence_penalty,\n\u001b[1;32m   1172\u001b[0m         repeat_penalty\u001b[39m=\u001b[39;49mrepeat_penalty,\n\u001b[1;32m   1173\u001b[0m         top_k\u001b[39m=\u001b[39;49mtop_k,\n\u001b[1;32m   1174\u001b[0m         stream\u001b[39m=\u001b[39;49mstream,\n\u001b[1;32m   1175\u001b[0m         tfs_z\u001b[39m=\u001b[39;49mtfs_z,\n\u001b[1;32m   1176\u001b[0m         mirostat_mode\u001b[39m=\u001b[39;49mmirostat_mode,\n\u001b[1;32m   1177\u001b[0m         mirostat_tau\u001b[39m=\u001b[39;49mmirostat_tau,\n\u001b[1;32m   1178\u001b[0m         mirostat_eta\u001b[39m=\u001b[39;49mmirostat_eta,\n\u001b[1;32m   1179\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m   1180\u001b[0m         stopping_criteria\u001b[39m=\u001b[39;49mstopping_criteria,\n\u001b[1;32m   1181\u001b[0m         logits_processor\u001b[39m=\u001b[39;49mlogits_processor,\n\u001b[1;32m   1182\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:1113\u001b[0m, in \u001b[0;36mLlama.create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m   1111\u001b[0m     chunks: Iterator[CompletionChunk] \u001b[39m=\u001b[39m completion_or_chunks\n\u001b[1;32m   1112\u001b[0m     \u001b[39mreturn\u001b[39;00m chunks\n\u001b[0;32m-> 1113\u001b[0m completion: Completion \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(completion_or_chunks)  \u001b[39m# type: ignore\u001b[39;00m\n\u001b[1;32m   1114\u001b[0m \u001b[39mreturn\u001b[39;00m completion\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:733\u001b[0m, in \u001b[0;36mLlama._create_completion\u001b[0;34m(self, prompt, suffix, max_tokens, temperature, top_p, logprobs, echo, stop, frequency_penalty, presence_penalty, repeat_penalty, top_k, stream, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, model, stopping_criteria, logits_processor)\u001b[0m\n\u001b[1;32m    731\u001b[0m finish_reason \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mlength\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    732\u001b[0m multibyte_fix \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m--> 733\u001b[0m \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgenerate(\n\u001b[1;32m    734\u001b[0m     prompt_tokens,\n\u001b[1;32m    735\u001b[0m     top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    736\u001b[0m     top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[1;32m    737\u001b[0m     temp\u001b[39m=\u001b[39mtemperature,\n\u001b[1;32m    738\u001b[0m     tfs_z\u001b[39m=\u001b[39mtfs_z,\n\u001b[1;32m    739\u001b[0m     mirostat_mode\u001b[39m=\u001b[39mmirostat_mode,\n\u001b[1;32m    740\u001b[0m     mirostat_tau\u001b[39m=\u001b[39mmirostat_tau,\n\u001b[1;32m    741\u001b[0m     mirostat_eta\u001b[39m=\u001b[39mmirostat_eta,\n\u001b[1;32m    742\u001b[0m     frequency_penalty\u001b[39m=\u001b[39mfrequency_penalty,\n\u001b[1;32m    743\u001b[0m     presence_penalty\u001b[39m=\u001b[39mpresence_penalty,\n\u001b[1;32m    744\u001b[0m     repeat_penalty\u001b[39m=\u001b[39mrepeat_penalty,\n\u001b[1;32m    745\u001b[0m     stopping_criteria\u001b[39m=\u001b[39mstopping_criteria,\n\u001b[1;32m    746\u001b[0m     logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    747\u001b[0m ):\n\u001b[1;32m    748\u001b[0m     \u001b[39mif\u001b[39;00m token \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_token_eos:\n\u001b[1;32m    749\u001b[0m         text \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdetokenize(completion_tokens)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:568\u001b[0m, in \u001b[0;36mLlama.generate\u001b[0;34m(self, tokens, top_k, top_p, temp, repeat_penalty, reset, frequency_penalty, presence_penalty, tfs_z, mirostat_mode, mirostat_tau, mirostat_eta, logits_processor, stopping_criteria)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreset()\n\u001b[1;32m    567\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 568\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meval(tokens)\n\u001b[1;32m    569\u001b[0m     token \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msample(\n\u001b[1;32m    570\u001b[0m         top_k\u001b[39m=\u001b[39mtop_k,\n\u001b[1;32m    571\u001b[0m         top_p\u001b[39m=\u001b[39mtop_p,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    580\u001b[0m         logits_processor\u001b[39m=\u001b[39mlogits_processor,\n\u001b[1;32m    581\u001b[0m     )\n\u001b[1;32m    582\u001b[0m     \u001b[39mif\u001b[39;00m stopping_criteria \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m stopping_criteria(\n\u001b[1;32m    583\u001b[0m         \u001b[39mlist\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_tokens), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_logits[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[1;32m    584\u001b[0m     ):\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama.py:311\u001b[0m, in \u001b[0;36mLlama.eval\u001b[0;34m(self, tokens)\u001b[0m\n\u001b[1;32m    309\u001b[0m n_past \u001b[39m=\u001b[39m \u001b[39mmin\u001b[39m(n_ctx \u001b[39m-\u001b[39m \u001b[39mlen\u001b[39m(batch), \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39meval_tokens))\n\u001b[1;32m    310\u001b[0m n_tokens \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(batch)\n\u001b[0;32m--> 311\u001b[0m return_code \u001b[39m=\u001b[39m llama_cpp\u001b[39m.\u001b[39;49mllama_eval(\n\u001b[1;32m    312\u001b[0m     ctx\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mctx,\n\u001b[1;32m    313\u001b[0m     tokens\u001b[39m=\u001b[39;49m(llama_cpp\u001b[39m.\u001b[39;49mllama_token \u001b[39m*\u001b[39;49m \u001b[39mlen\u001b[39;49m(batch))(\u001b[39m*\u001b[39;49mbatch),\n\u001b[1;32m    314\u001b[0m     n_tokens\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_tokens),\n\u001b[1;32m    315\u001b[0m     n_past\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(n_past),\n\u001b[1;32m    316\u001b[0m     n_threads\u001b[39m=\u001b[39;49mllama_cpp\u001b[39m.\u001b[39;49mc_int(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_threads),\n\u001b[1;32m    317\u001b[0m )\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m return_code \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    319\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mllama_eval returned \u001b[39m\u001b[39m{\u001b[39;00mreturn_code\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/llama_cpp/llama_cpp.py:439\u001b[0m, in \u001b[0;36mllama_eval\u001b[0;34m(ctx, tokens, n_tokens, n_past, n_threads)\u001b[0m\n\u001b[1;32m    432\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mllama_eval\u001b[39m(\n\u001b[1;32m    433\u001b[0m     ctx: llama_context_p,\n\u001b[1;32m    434\u001b[0m     tokens,  \u001b[39m# type: Array[llama_token]\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    437\u001b[0m     n_threads: c_int,\n\u001b[1;32m    438\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[0;32m--> 439\u001b[0m     \u001b[39mreturn\u001b[39;00m _lib\u001b[39m.\u001b[39;49mllama_eval(ctx, tokens, n_tokens, n_past, n_threads)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "rewards = []\n",
    "\n",
    "for i_episode in range(2):\n",
    "    observation = env.reset()\n",
    "\n",
    "    episode_reward = 0\n",
    "\n",
    "    # Loop over t timesteps \n",
    "    for t in range(5):\n",
    "\n",
    "        print(f\"=> TIMESTAMP [{t}]; EPISODE [{i_episode}]<=\")\n",
    "\n",
    "        position = observation[0]\n",
    "\n",
    "        # save observation\n",
    "        agent.save_observation(f\"you are in position {position}\")\n",
    "        print(f\"saved observation: you are in position {position}\")\n",
    "\n",
    "        # generate action\n",
    "        action = agent.generate_action(debug=True)\n",
    "        print(\"selected action\", action)\n",
    "\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        # save reward\n",
    "        agent.save_reward(reward)\n",
    "\n",
    "        # If the episode terminated prematurely, save the reward and stop the episode\n",
    "        if terminated:\n",
    "            print(\"Episode finished after {} timesteps\".format(t+1))\n",
    "            rewards.append(reward)\n",
    "            break\n",
    "\n",
    "        if t == 9:\n",
    "            rewards.append(reward)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f961e878700>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(rewards)\n",
    "plt.plot(rewards)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
